{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fdcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imported usefullibraries and chrome web driver installed\n",
    "import pandas as pd\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#extract 100 links from Amazon_scraping file and store in pandas\n",
    "df = pd.read_excel(r'Amazon_Scraping.xlsx')\n",
    "File = pd.DataFrame(df)\n",
    "web_srapper_Data = File.loc[10:110]\n",
    "Asin = web_srapper_Data['Asin']\n",
    "country = web_srapper_Data['country']\n",
    "Asin = Asin.astype(str)\n",
    "ID = web_srapper_Data['id']\n",
    "\n",
    "\n",
    "List_of_products = [] \n",
    "\n",
    "'''get Asin and country value to create link with loop'''\n",
    "for i,j in zip(country,Asin):\n",
    "    \n",
    "    url = \"https://www.amazon.\"+i+\"/dp/\"+j\n",
    "    \n",
    "    # installl web driver \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get(url)\n",
    "    # soup and parse the html content of amazon link\n",
    "    soup =BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # counting of div tags in not working link and working links\n",
    "    results_1 = soup.find('div')\n",
    "    div_len=len(results_1)\n",
    "    \n",
    "    #results_1 = soup.find('a')['href']\n",
    "    #results_2 = soup.find('title').get_text()\n",
    "    #if results_1 == '/ref=cs_404_logo/'or results_2 == ('404 - Documento non trovato'or'404 - Documento no encontrado'or'Page introuvable'):\n",
    "    \n",
    "    #----this section will work for the links which is working----\n",
    "    if div_len <3:\n",
    "     \n",
    "        key = ['title','price','details','img_url']\n",
    "        value = []\n",
    "        Dict = {}\n",
    "        soup =BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        #this code will fetch the title from the link\n",
    "        title = soup.title.string\n",
    "        value.append(title)#value store in the list\n",
    "\n",
    "        ##this code will fetch the product price from the link \n",
    "        results = soup.find_all('span', class_=('a-size-base a-color-price a-color-price'))\n",
    "        try:\n",
    "            if results ==[]:\n",
    "                results1 = soup.find('span', class_='a-size-base a-color-price')\n",
    "\n",
    "                if results1 ==([] or None):\n",
    "                    try:\n",
    "                        results2 = soup.find_all('span', class_='a-color-base')\n",
    "                    except:\n",
    "                        results2 = soup.find('span', class_='a-color-base')\n",
    "                    item2 = results2[1]\n",
    "                    item3 = item2.text.strip()\n",
    "                    value.append(item3)\n",
    "                else:\n",
    "                    item1 = results1\n",
    "                    item2 = item1.text\n",
    "                    value.append(item2)\n",
    "            else:\n",
    "                item = results\n",
    "                item1 = item.text\n",
    "                value.append(item1)\n",
    "        except:\n",
    "            results3 = soup.find('span', class_='a-size-base a-color-price a-color-price')\n",
    "            item = results3\n",
    "            item1 = item.text\n",
    "            value.append(item1)#value store in the list\n",
    "\n",
    "            \n",
    "        ##this code will fetch the product details from the link\n",
    "        soup =BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        try:\n",
    "            results = soup.find_all('div', id='detailBullets_feature_div')\n",
    "            item = results[1]\n",
    "            item1 = item.text.replace('\\u200e','')\n",
    "            item2 = item1.replace('\\n','')\n",
    "            item3 = item2.replace('\\u200f','')\n",
    "            item4 = item3.replace('  ','')\n",
    "            value.append(item4)\n",
    "        except:\n",
    "            exception= 'no details mentioned'\n",
    "            value.append(exception) #value store in the list\n",
    "           \n",
    "\n",
    "        #this code will fetch the url of image from the link\n",
    "        results = soup.find('div', id=\"img-canvas\")\n",
    "        if results == None:\n",
    "            results = soup.find('div', id=\"imgTagWrapperId\")\n",
    "            item = results.children\n",
    "            item1 = list(item)\n",
    "            img_link = item1[1]['src']\n",
    "            value.append(img_link) #value store in the list\n",
    "            \n",
    "        \n",
    "\n",
    "        product_dict = dict(zip(key,value)) #zipping the dictionaries with value stored in list and keys\n",
    "        Dict.update(product_dict) #updating the value in the dictionary\n",
    "        List_of_products.append(Dict)# store the dictionary value in final list\n",
    "        print(Dict) # printing the value to check output is right\n",
    "     \n",
    "    \n",
    "    #----this section will work for those links are not working----\n",
    "    elif div_len != 0:\n",
    "        \n",
    "        # declare the values\n",
    "        key1 = []\n",
    "        value1 = []\n",
    "        Dict1 = {} \n",
    "        \n",
    "        \n",
    "        NA = 'not available'\n",
    "        key1.append(url) # stored url which is not working\n",
    "        value1.append(NA) # stored value 'not available'\n",
    "        product_dict = dict(zip(key1,value1))# url and value is zipping in the dictinary\n",
    "        Dict1.update(product_dict) # stored zipping value in the dictionary\n",
    "        List_of_products.append(Dict1) # stored the dictionary in final list\n",
    "        print(Dict1)\n",
    "    else:\n",
    "        # this code will work if previous code will not work if div_len value =0\n",
    "        NA = 'not available'\n",
    "        key1.append(url)\n",
    "        value1.append(NA)\n",
    "        product_dict = dict(zip(key1,value1))\n",
    "        Dict1.update(product_dict)\n",
    "        List_of_products.append(Dict1)\n",
    "        print(Dict1)\n",
    "        \n",
    "#convert data into Json\n",
    "        \n",
    "import json\n",
    "\n",
    "List_of_products\n",
    "json_string = json.dumps(List_of_products)\n",
    "\n",
    "# Writing to sample.json\n",
    "with open(\"resultant.json\", \"w\") as outfile:\n",
    "    outfile.write(resultant.json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
